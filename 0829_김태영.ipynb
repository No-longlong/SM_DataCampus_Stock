{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크롤링으로  업종별 코드 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['234', '138', '173', '230', '42', '44', '227', '136', '196', '212', '36', '210', '134', '229', '144', '171', '232', '177', '191', '192', '12', '176', '20', '146', '154', '161', '168', '198', '190', '207', '226', '174', '215', '62', '185', '189', '199', '217', '109', '38', '187', '164', '203', '205', '216', '25', '204', '208', '162', '235', '197', '53', '35', '140', '219', '206', '220', '200', '167', '33', '145', '231', '228', '193', '221', '183', '214', '182', '202', '213', '211', '222', '233', '184', '236', '194', '166', '218', '201']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib.request\n",
    "from selenium.webdriver import Chrome\n",
    "import json\n",
    "import re\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import datetime as dt\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "# selenium을 위한 chrome 브라우저 켜기\n",
    "browser  = Chrome()\n",
    "browser.maximize_window()\n",
    "\n",
    "annual_date = ['2015/12', '2016/12', '2017/12', '2018/12', '2019/12', '2020/12','2021/12', '2022/12']\n",
    "# 각각 지표들 데이터프레임에 넣기 위한 초기 선언, (df_NI(Net Income) : 당기순이익을 뜻함)\n",
    "\n",
    "url_up = 'https://finance.naver.com/sise/sise_group.nhn?type=upjong'\n",
    "\n",
    "## 밑에는 업종별 코드를 반환하기 위한 코드 \n",
    "res = requests.get(url_up)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "upjong = {}\n",
    "rows = soup.select('td > a')\n",
    "\n",
    "for r in range(len(rows)):\n",
    "    upjong[soup.select('td > a')[r]['href'][43:]] = soup.select('td > a')[r].text\n",
    "upjong_code = list(upjong.keys())[:-1]\n",
    "print(upjong_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 업종코드를 입력하면 --> 해당 기업 코드가 나오는 함수. \n",
    "def upjong_url(code):\n",
    "    tickers = {}\n",
    "    url_tmp = \"https://finance.naver.com/sise/sise_group_detail.nhn?type=upjong&no=%s\"\n",
    "    url = url_tmp % (code)\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    \n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    rows = soup.select('td > div')\n",
    "\n",
    "    for r in rows[1:]:\n",
    "        tickers[r.select('a')[0]['href'][20:]] = r.text\n",
    "    a = [k for k in tickers.keys() if tickers[k][-2]=='우' or tickers[k][-3]=='우']\n",
    "    # 우량주 종목 제외    \n",
    "    for i in range(len(a)):\n",
    "        del tickers[a[i]]\n",
    "\n",
    "    code = list(tickers.keys())\n",
    "    com = list(tickers.values())\n",
    "    return code,com\n",
    "#회사 코드를 사용해서 url완성하는 함수\n",
    "def ttt(xx):\n",
    "    name = xx\n",
    "    base_url = 'https://finance.naver.com/item/coinfo.nhn?code='+ name + '&target=finsum_more'\n",
    "    return base_url\n",
    "# 종목코드를 통해 재무제표 크롤링\n",
    "def stock_crawler(code):\n",
    "    #code = 종목번호\n",
    "    name = code\n",
    "    base_url = 'https://finance.naver.com/item/coinfo.nhn?code='+ name + '&target=finsum_more'\n",
    "    \n",
    "    browser.get(base_url)\n",
    "    #frmae구조 안에 필요한 데이터가 있기 때문에 해당 데이터를 수집하기 위해서는 frame구조에 들어가야한다.\n",
    "    browser.switch_to_frame(browser.find_element_by_id('coinfo_cp'))\n",
    "    \n",
    "    #재무제표 \"연간\" 클릭하기\n",
    "    browser.find_elements_by_xpath('//*[@class=\"schtab\"][1]/tbody/tr/td[3]')[0].click()\n",
    "\n",
    "    html0 = browser.page_source\n",
    "    html1 = BeautifulSoup(html0,'html.parser')\n",
    "    \n",
    "    #기업명 뽑기\n",
    "    title0 = html1.find('head').find('title').text\n",
    "    #print(title0.split('-')[-1])\n",
    "    \n",
    "    html22 = html1.find('table',{'class':'gHead01 all-width','summary':'주요재무정보를 제공합니다.'})\n",
    "    \n",
    "    #date scrapy\n",
    "    thead0 = html22.find('thead')\n",
    "    tr0 = thead0.find_all('tr')[1]\n",
    "    th0 = tr0.find_all('th')\n",
    "    \n",
    "    date = []\n",
    "    for i in range(len(th0)):\n",
    "        date.append(''.join(re.findall('[0-9/]',th0[i].text)))\n",
    "    \n",
    "    #columns scrapy\n",
    "    tbody0 = html22.find('tbody')\n",
    "    tr0 = tbody0.find_all('tr')\n",
    "    \n",
    "    col = []\n",
    "    for i in range(len(tr0)):\n",
    "\n",
    "        if '\\xa0' in tr0[i].find('th').text:\n",
    "            tx = re.sub('\\xa0','',tr0[i].find('th').text)\n",
    "        else:\n",
    "            tx = tr0[i].find('th').text\n",
    "\n",
    "        col.append(tx)\n",
    "    \n",
    "    #main text scrapy\n",
    "    td = []\n",
    "    for i in range(len(tr0)):\n",
    "        td0 = tr0[i].find_all('td')\n",
    "        td1 = []\n",
    "        for j in range(len(td0)):\n",
    "            if td0[j].text == '':\n",
    "                td1.append('0')\n",
    "            else:\n",
    "                td1.append(td0[j].text)\n",
    "\n",
    "        td.append(td1)\n",
    "    \n",
    "    td2 = list(map(list,zip(*td)))\n",
    "    \n",
    "    return pd.DataFrame(td2,columns = col,index = date).T\n",
    "# numpy -> pandas 데이터프레임으로 변환하는 사용자 정의 함수\n",
    "def trans_df(ar,com):\n",
    "    #df = np.array([])\n",
    "     \n",
    "    ar.resize((int(len(ar)/8),8))\n",
    "    df = pd.DataFrame(data = ar,index=com,columns = annual_date)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\No\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: use driver.switch_to.frame instead\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-36883e02a147>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# 재무제표를 종목별로 불러와 지표 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstock_crawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mdf_PER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_PER\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PER(배)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdf_PBR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_PBR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PBR(배)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8dfcb1729339>\u001b[0m in \u001b[0;36mstock_crawler\u001b[1;34m(code)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#재무제표 \"연간\" 클릭하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//*[@class=\"schtab\"][1]/tbody/tr/td[3]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mhtml0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# res가 업종별 5순위 종목을 누적해서 데이터프레임에 저장 업종코드 res라는 전혀 다른 의미.\n",
    "\n",
    "res = pd.DataFrame([])\n",
    "toplist = []\n",
    "for up in upjong_code:\n",
    "    # 업종별로 회사 코드를 받음, 전자장비및기기업종 -> 78개 회사 리스트를 받는다.\n",
    "    print(up)\n",
    "    df_PER,df_PBR,df_ROE,df_ROA,df_EPS,df_NI = np.array([]),np.array([]),np.array([]),np.array([]),np.array([]),np.array([])\n",
    "    code, com = upjong_url(up)\n",
    "    for c in code:\n",
    "        # 재무제표를 종목별로 불러와 지표 저장\n",
    "        df = stock_crawler(c)\n",
    "        df_PER = np.hstack([df_PER,df.loc['PER(배)'].values])\n",
    "        df_PBR = np.hstack([df_PBR,df.loc['PBR(배)'].values])\n",
    "        df_EPS = np.hstack([df_EPS,df.loc['EPS(원)'].values])\n",
    "        df_ROE = np.hstack([df_ROE,df.loc['ROE(%)'].values])\n",
    "        df_ROA = np.hstack([df_ROA,df.loc['ROA(%)'].values])\n",
    "        df_NI = np.hstack([df_NI,df.loc['당기순이익'].values])\n",
    "    # 데이터프레임으로 변환\n",
    "    df_PER = trans_df(df_PER,com)\n",
    "    df_PBR = trans_df(df_PBR,com)\n",
    "    df_EPS = trans_df(df_EPS,com)\n",
    "    df_ROE = trans_df(df_ROE,com)\n",
    "    df_ROA = trans_df(df_ROA,com)\n",
    "    df_NI = trans_df(df_NI,com)\n",
    "    # 지표별로 랭크 구하여 합산하기\n",
    "    df_rank = df_PER.rank(method='min')+df_PBR.rank(method='min')+df_EPS.rank(method='max')+df_ROE.rank(method='max')+df_ROA.rank(method='max')\n",
    "    +df_NI.rank(method='max')\n",
    "    # 연도\n",
    "    sc ='2019/12'\n",
    "    df_rank.sort_values(by=sc,inplace=True)\n",
    "    res = pd.concat([res,df_rank.head()])\n",
    "    # 업종별로 랭크가 작은 상위 5위권 항목들 리스트 보완해야 함.\n",
    "    #toplist.append(list(df_rank.head().index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toplist = list(np.array(toplist).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
